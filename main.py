import torch, argparse, json
from transformers import AutoTokenizer, AutoModelForCausalLM

parser = argparse.ArgumentParser()
parser.add_argument("--model", default="THUDM/glm-4-9b-chat-hf")
args = parser.parse_args()

tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    args.model, device_map="auto",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.bfloat16,
    trust_remote_code=True
).eval()

history = []  # å­˜å‚¨å¤šè½®å¯¹è¯

def build_prompt(history, user_msg):
    """å°†å†å²é—®ç­”ä¸²æˆç¬¦åˆ GLMâ€‘4 çš„æ ¼å¼"""
    prompt = ""
    for turn in history:
        prompt += f"<|user|>\n{turn['user']}<|endoftext|>\n<|assistant|>\n{turn['assistant']}<|endoftext|>\n"
    prompt += f"<|user|>\n{user_msg}<|endoftext|>\n<|assistant|>\n"
    return prompt

while True:
    user_msg = input("ğŸ§‘â€ğŸ’» ä½ ï¼š").strip()
    if user_msg.lower() in {"exit", "quit"}:
        break
    input_ids = tokenizer(build_prompt(history, user_msg),
                          return_tensors="pt").input_ids.to(model.device)
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.8,
            eos_token_id=tokenizer.eos_token_id
        )
    answer = tokenizer.decode(output_ids[0, input_ids.shape[-1]:],
                              skip_special_tokens=True)
    print("ğŸ¤– åŠ©æ‰‹ï¼š", answer)
    history.append({"user": user_msg, "assistant": answer})